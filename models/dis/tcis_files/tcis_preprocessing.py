# -*- coding: utf-8 -*-
"""system_independent_analysis_similar_name.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WHeZwGRIsSfVDvBNl6IYrgIFUAlbUpg3
"""

import pandas as pd
import numpy as np
import itertools
import pickle
import os
import sklearn
# from sklearn.model_selection import train_test_split
# from imblearn.over_sampling import RandomOverSampler
# from sklearn.preprocessing import MinMaxScaler
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import accuracy_score
# from sklearn.metrics import precision_score
# from sklearn.metrics import f1_score
# from sklearn.metrics import recall_score
# from sklearn.metrics import classification_report
# from sklearn.metrics import roc_curve, auc
# from sklearn import svm
# import xgboost as xgb
# import lightgbm as lgb
# from sklearn.ensemble import AdaBoostClassifier
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout
# from tensorflow.keras.utils import to_categorical
# from sklearn.ensemble import RandomForestClassifier, VotingClassifier
# from sklearn.preprocessing import LabelBinarizer
# import matplotlib.pyplot as plt
# from sklearn.metrics import RocCurveDisplay
# from itertools import cycle
# from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
# from sklearn.metrics import PrecisionRecallDisplay

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/MyDrive/Colab\ Notebooks
def tcis_preprocessing(df):
    df = pd.read_csv('summer/realtime/ultimate_data.csv')
    df = df.drop(columns = ['Unnamed: 0','datetime'])
    similar_names = ['connections_info','num_handles','nonpaged_pool','pagefile','paged_pool',
                    'peak_pagefile','peak_nonpaged_pool','peak_wset','read_count','user',
                    'system','nice','wset','private','cnt','num_page_faults','vms','memory_percent',
                    'rss','read_bytes','write_bytes','write_count','ionice','other_count']
    for similar_name in similar_names:
        df[similar_name] = df.filter(like=similar_name).sum(axis=1)
        df = df[df.columns.drop(list(df.filter(regex='_'+similar_name)))]
    df['label'] = df['label'].astype('string')
    return df


    # df.to_csv('source_files/attack_dis_sim.csv')

# df.info(verbose=True)

# df.head()

# df = df[df['label']!='normal']
# X = df.drop(columns = ['label'])
# y = df['label']
# ros = RandomOverSampler(sampling_strategy="not majority")
# X_res, y_res = ros.fit_resample(X,y)
# X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.1, random_state = 42, stratify = y_res)
# scaler = MinMaxScaler()
# scaler.fit(X_train)
# X_train = scaler.transform(X_train)
# X_test = scaler.transform(X_test)
# y_train, y_test = y_train.values, y_test.values
# num_classes = len(np.unique(y_train))

# rf_model = RandomForestClassifier(n_estimators=1000, max_depth = 10, criterion = 'entropy', random_state = 42)
# rf_model.fit(X_train, y_train)
# rf_hat = rf_model.predict(X_test)
# print('rf_model')
# print(precision_score(rf_hat, y_test, average='weighted'))
# print(f1_score(rf_hat, y_test, average='weighted'))
# print(recall_score(rf_hat, y_test, average='weighted'))
# print(classification_report(rf_hat, y_test))

# lgb_model = lgb.LGBMClassifier(n_estimators=500, max_depth = 6, learning_rate = 0.01,random_state = 42)
# lgb_model.fit(X_train, y_train)
# lgb_hat = lgb_model.predict(X_test)
# print('lgb_model')
# print(lgb_model.score(X_test, y_test))
# print(precision_score(lgb_hat, y_test, average='weighted'))
# print(f1_score(lgb_hat, y_test, average='weighted'))
# print(recall_score(lgb_hat, y_test, average='weighted'))
# print(classification_report(lgb_hat, y_test))

# from sklearn.preprocessing import LabelEncoder
# le = LabelEncoder()
# y_train = le.fit_transform(y_train)

# xgb_model = xgb.XGBClassifier(n_estimators=250, max_depth = 2, learning_rate = 0.1, random_state = 42)
# xgb_model.fit(X_train, y_train)
# xgb_hat = xgb_model.predict(X_test)
# print('xgb_model')
# print(xgb_model.score(X_test, y_test))
# print(precision_score(xgb_hat, y_test, average='weighted'))
# print(f1_score(xgb_hat, y_test, average='weighted'))
# print(recall_score(xgb_hat, y_test, average='weighted'))
# print(classification_report(xgb_hat, y_test))

# svm_model = svm.SVC(C=1, kernel = "rbf", degree = 3, random_state = 42)
# svm_model.fit(X_train, y_train)
# svm_hat = svm_model.predict(X_test)
# print('svm_model')
# print(svm_model.score(X_test, y_test))
# print(precision_score(svm_hat, y_test, average='weighted'))
# print(f1_score(svm_hat, y_test, average='weighted'))
# print(recall_score(svm_hat, y_test, average='weighted'))
# print(classification_report(svm_hat, y_test))

# ada_model = AdaBoostClassifier(n_estimators=100, algorithm = "SAMME.R", learning_rate = 0.01, random_state = 42)
# ada_model.fit(X_train, y_train)
# ada_hat = ada_model.predict(X_test)
# print('ada_model')
# print(ada_model.score(X_test, y_test))
# print(precision_score(ada_hat, y_test, average='weighted'))
# print(f1_score(ada_hat, y_test, average='weighted'))
# print(recall_score(ada_hat, y_test, average='weighted'))
# print(classification_report(ada_hat, y_test))

# attack_labels = df['label'].unique()

# cm = confusion_matrix(y_test, rf_hat)
# cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = attack_labels)
# fig, ax = plt.subplots(figsize=(10,10), dpi=300)
# plt.rcParams.update({'font.size': 14,'axes.labelweight':'bold','font.weight':'bold'})
# cmp.plot(xticks_rotation='vertical', cmap='binary',ax=ax)
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/results/system_independent.jpg',bbox_inches="tight")

# cm = confusion_matrix(y_test, lgb_hat)
# cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = attack_labels)
# fig, ax = plt.subplots(figsize=(10,10),dpi=300)
# plt.rcParams.update({'font.size': 14,'axes.labelweight':'bold','font.weight':'bold'})
# cmp.plot(xticks_rotation='vertical', cmap='binary',ax=ax)
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/lgb_cm.jpg',bbox_inches="tight")

# cm = confusion_matrix(y_test, xgb_hat)
# cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = attack_labels)
# fig, ax = plt.subplots(figsize=(10,10), dpi=300)
# plt.rcParams.update({'font.size': 14,'axes.labelweight':'bold','font.weight':'bold'})
# cmp.plot(xticks_rotation='vertical', cmap='binary',ax=ax)
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/xgb_cm.jpg',bbox_inches="tight")

# cm = confusion_matrix(y_test, svm_hat)
# cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = attack_labels)
# fig, ax = plt.subplots(figsize=(10,10), dpi=300)
# plt.rcParams.update({'font.size': 14,'axes.labelweight':'bold','font.weight':'bold'})
# cmp.plot(xticks_rotation='vertical', cmap='binary',ax=ax)
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/svm_cm.jpg',bbox_inches="tight")

# cm = confusion_matrix(y_test, ada_hat)
# cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = attack_labels)
# fig, ax = plt.subplots(figsize=(10,10),dpi=300)
# plt.rcParams.update({'font.size': 14,'axes.labelweight':'bold','font.weight':'bold'})
# cmp.plot(xticks_rotation='vertical', cmap='binary',ax=ax)
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/ada_cm.jpg',bbox_inches="tight")

# """# **Majority vote classifier**"""

# # clf1 = RandomForestClassifier(n_estimators=1000, max_depth = 10, criterion = 'entropy', random_state = 42)
# # clf2 = lgb.LGBMClassifier(n_estimators=500, max_depth = 6, learning_rate = 0.01, random_state = 42)
# # clf3 = xgb.XGBClassifier(n_estimators=250, max_depth = 2, learning_rate = 0.1, random_state = 42)
# # clf4 = AdaBoostClassifier(n_estimators=100, algorithm = "SAMME.R", learning_rate = 0.01, random_state = 42)
# # clf5 = svm.SVC(C=1, kernel = "rbf", degree = 3, random_state = 42)
# #eclf1 = VotingClassifier(estimators=[('rf', clf1), ('lgb', clf2), ('xgb', clf3), ('ada', clf4),('svm', clf5)], voting='hard')
# eclf1 = VotingClassifier(estimators=[('rf', rf_model), ('lgb', lgb_model), ('xgb', xgb_model), ('ada', ada_model),('svm', svm_model)], voting='hard')
# eclf1 = eclf1.fit(X_train, y_train)
# eclf1_hat = eclf1.predict(X_test)
# print('majority_vote_model')
# print(eclf1.score(X_test, y_test))
# print(precision_score(eclf1_hat, y_test, average='weighted'))
# print(f1_score(eclf1_hat, y_test, average='weighted'))
# print(recall_score(eclf1_hat, y_test, average='weighted'))
# print(classification_report(eclf1_hat, y_test))

# cm = confusion_matrix(y_test, eclf1_hat)
# cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = attack_labels)
# fig, ax = plt.subplots(figsize=(10,10),  dpi=300)
# plt.rcParams.update({'font.size': 14,'axes.labelweight':'bold','font.weight':'bold'})
# cmp.plot(xticks_rotation='vertical', cmap='binary',ax=ax)
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/majority_vote_cm.jpg',bbox_inches="tight")

# eclf0 = VotingClassifier(estimators=[('rf', rf_model), ('ada', ada_model),('svm', svm_model)], voting='hard')
# eclf0 = eclf0.fit(X_train, y_train)
# eclf0_hat = eclf0.predict(X_test)
# print('majority_vote_model')
# print(eclf0.score(X_test, y_test))
# print(precision_score(eclf0_hat, y_test, average='weighted'))
# print(f1_score(eclf0_hat, y_test, average='weighted'))
# print(recall_score(eclf0_hat, y_test, average='weighted'))
# print(classification_report(eclf0_hat, y_test))

# cm = confusion_matrix(y_test, eclf0_hat)
# cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = attack_labels)
# fig, ax = plt.subplots(figsize=(10,10),  dpi=300)
# plt.rcParams.update({'font.size': 14,'axes.labelweight':'bold','font.weight':'bold'})
# cmp.plot(xticks_rotation='vertical', cmap='binary',ax=ax)
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/majority_vote_cm.jpg',bbox_inches="tight")

# eclf2 = VotingClassifier(estimators=[('rf', rf_model), ('lgb', lgb_model), ('xgb', xgb_model)], voting='hard')
# eclf2 = eclf2.fit(X_train, y_train)
# eclf2_hat = eclf2.predict(X_test)
# print('majority_vote_model')
# print(eclf2.score(X_test, y_test))
# print(precision_score(eclf2_hat, y_test, average='weighted'))
# print(f1_score(eclf2_hat, y_test, average='weighted'))
# print(recall_score(eclf2_hat, y_test, average='weighted'))
# print(classification_report(eclf2_hat, y_test))

# cm = confusion_matrix(y_test, eclf2_hat)
# cmp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = attack_labels)
# fig, ax = plt.subplots(figsize=(10,10),dpi=300)
# plt.rcParams.update({'font.size': 14,'axes.labelweight':'bold','font.weight':'bold'})
# cmp.plot(xticks_rotation='vertical', cmap='binary',ax=ax)
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/majority_vote_cm.jpg',bbox_inches="tight")

# """### **ROC curves**"""

# rf_decision = rf_model.predict_proba(X_test)
# lgb_decision = lgb_model.predict_proba(X_test)
# xgb_decision = xgb_model.predict_proba(X_test)
# svm_decision = svm_model.decision_function(X_test)
# ada_decision = ada_model.decision_function(X_test)
# majority_vote_decision = eclf1.predict_proba(X_test)

# eclf3 = VotingClassifier(estimators=[('rf', rf_model), ('lgb', lgb_model), ('xgb', xgb_model)], voting='soft')
# eclf3 = eclf3.fit(X_train, y_train)

# majority_vote_decision = eclf3.predict_proba(X_test)

# label_binarizer = LabelBinarizer().fit(y_train)
# y_onehot_test = label_binarizer.transform(y_test)

# fig, ax = plt.subplots(figsize=(12, 12))

# colors = cycle(["aqua", "darkorange", "cornflowerblue","brown","teal","purple",
#                 "greenyellow","lightsalmon","saddlebrown","indigo","orange",
#                 "silver","darkred","mistyrose","darkkhaki","crimson"])

# for class_id, color in zip(range(16), colors):
#     RocCurveDisplay.from_predictions(
#         y_onehot_test[:, class_id],
#         rf_decision[:, class_id],
#         name=f"ROC curve for {attack_labels[class_id]}",
#         color=color,
#         ax=ax,
#     )

# plt.plot([0, 1], [0, 1], "k--", label="ROC curve for chance level (AUC = 0.5)")
# plt.axis("square")
# plt.xlabel("False Positive Rate")
# plt.ylabel("True Positive Rate")
# plt.legend()
# plt.rcParams.update({'font.size': 12,'axes.labelweight':'bold','font.weight':'bold'})
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/rf_roc.jpg',bbox_inches="tight")
# plt.show()

# fig, ax = plt.subplots(figsize=(12, 12))

# colors = cycle(["aqua", "darkorange", "cornflowerblue","brown","teal","purple",
#                 "greenyellow","lightsalmon","saddlebrown","indigo","orange",
#                 "silver","darkred","mistyrose","darkkhaki","crimson"])

# for class_id, color in zip(range(16), colors):
#     RocCurveDisplay.from_predictions(
#         y_onehot_test[:, class_id],
#         lgb_decision[:, class_id],
#         name=f"ROC curve for {attack_labels[class_id]}",
#         color=color,
#         ax=ax,
#     )

# plt.plot([0, 1], [0, 1], "k--", label="ROC curve for chance level (AUC = 0.5)")
# plt.axis("square")
# plt.xlabel("False Positive Rate")
# plt.ylabel("True Positive Rate")
# plt.legend()
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/lgb_roc.jpg',bbox_inches="tight")
# plt.show()

# fig, ax = plt.subplots(figsize=(12, 12))

# colors = cycle(["aqua", "darkorange", "cornflowerblue","brown","teal","purple",
#                 "greenyellow","lightsalmon","saddlebrown","indigo","orange",
#                 "silver","darkred","mistyrose","darkkhaki","crimson"])

# for class_id, color in zip(range(16), colors):
#     RocCurveDisplay.from_predictions(
#         y_onehot_test[:, class_id],
#         rf_decision[:, class_id],
#         name=f"ROC curve for {attack_labels[class_id]}",
#         color=color,
#         ax=ax,
#     )

# plt.plot([0, 1], [0, 1], "k--", label="ROC curve for chance level (AUC = 0.5)")
# plt.axis("square")
# plt.xlabel("False Positive Rate")
# plt.ylabel("True Positive Rate")
# plt.legend()
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/rf_roc.jpg',bbox_inches="tight")
# plt.show()

# fig, ax = plt.subplots(figsize=(12, 12))

# colors = cycle(["aqua", "darkorange", "cornflowerblue","brown","teal","purple",
#                 "greenyellow","lightsalmon","saddlebrown","indigo","orange",
#                 "silver","darkred","mistyrose","darkkhaki","crimson"])

# for class_id, color in zip(range(16), colors):
#     RocCurveDisplay.from_predictions(
#         y_onehot_test[:, class_id],
#         xgb_decision[:, class_id],
#         name=f"ROC curve for {attack_labels[class_id]}",
#         color=color,
#         ax=ax,
#     )

# plt.plot([0, 1], [0, 1], "k--", label="ROC curve for chance level (AUC = 0.5)")
# plt.axis("square")
# plt.xlabel("False Positive Rate")
# plt.ylabel("True Positive Rate")
# plt.legend()
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/xgb_roc.jpg',bbox_inches="tight")
# plt.show()

# fig, ax = plt.subplots(figsize=(12, 12))

# colors = cycle(["aqua", "darkorange", "cornflowerblue","brown","teal","purple",
#                 "greenyellow","lightsalmon","saddlebrown","indigo","orange",
#                 "silver","darkred","mistyrose","darkkhaki","crimson"])

# for class_id, color in zip(range(16), colors):
#     RocCurveDisplay.from_predictions(
#         y_onehot_test[:, class_id],
#         svm_decision[:, class_id],
#         name=f"ROC curve for {attack_labels[class_id]}",
#         color=color,
#         ax=ax,
#     )

# plt.plot([0, 1], [0, 1], "k--", label="ROC curve for chance level (AUC = 0.5)")
# plt.axis("square")
# plt.xlabel("False Positive Rate")
# plt.ylabel("True Positive Rate")
# plt.legend()
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/svm_roc.jpg',bbox_inches="tight")
# plt.show()

# fig, ax = plt.subplots(figsize=(12, 12))

# colors = cycle(["aqua", "darkorange", "cornflowerblue","brown","teal","purple",
#                 "greenyellow","lightsalmon","saddlebrown","indigo","orange",
#                 "silver","darkred","mistyrose","darkkhaki","crimson"])

# for class_id, color in zip(range(16), colors):
#     RocCurveDisplay.from_predictions(
#         y_onehot_test[:, class_id],
#         ada_decision[:, class_id],
#         name=f"ROC curve for {attack_labels[class_id]}",
#         color=color,
#         ax=ax,
#     )

# plt.plot([0, 1], [0, 1], "k--", label="ROC curve for chance level (AUC = 0.5)")
# plt.axis("square")
# plt.xlabel("False Positive Rate")
# plt.ylabel("True Positive Rate")
# plt.legend()
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/ada_roc.jpg',bbox_inches="tight")
# plt.show()

# fig, ax = plt.subplots(figsize=(12, 12))

# colors = cycle(["aqua", "darkorange", "cornflowerblue","brown","teal","purple",
#                 "greenyellow","lightsalmon","saddlebrown","indigo","orange",
#                 "silver","darkred","mistyrose","darkkhaki","crimson"])

# for class_id, color in zip(range(16), colors):
#     RocCurveDisplay.from_predictions(
#         y_onehot_test[:, class_id],
#         majority_vote_decision[:, class_id],
#         name=f"ROC curve for {attack_labels[class_id]}",
#         color=color,
#         ax=ax,
#     )

# plt.plot([0, 1], [0, 1], "k--", label="ROC curve for chance level (AUC = 0.5)")
# plt.axis("square")
# plt.xlabel("False Positive Rate")
# plt.ylabel("True Positive Rate")
# plt.legend()
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/majority_vote_roc.jpg',bbox_inches="tight")
# plt.show()

# """## **Precision vs recall**"""

# from sklearn.preprocessing import label_binarize
# from sklearn.pipeline import make_pipeline
# from sklearn.preprocessing import StandardScaler
# from sklearn.svm import LinearSVC
# from sklearn.multiclass import OneVsRestClassifier
# from sklearn.metrics import precision_recall_curve
# from sklearn.metrics import average_precision_score
# from sklearn.metrics import PrecisionRecallDisplay

# df = pd.read_csv('summer/realtime/ultimate_data.csv')
# attack_labels = df['label'].unique()
# df.drop(['Unnamed: 0','datetime',],inplace=True, axis=1)
# df = df[df['label']!='normal']

# X = df.drop(columns = ['label'])
# y = df['label']

# ros = RandomOverSampler(sampling_strategy="not majority")
# X_res, y_res = ros.fit_resample(X,y)

# X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 0.1, random_state = 42, stratify = y_res)
# scaler = MinMaxScaler()
# scaler.fit(X_train)
# X_train = scaler.transform(X_train)
# X_test = scaler.transform(X_test)
# y_train, y_test = y_train.values, y_test.values
# num_classes = len(np.unique(y_train))

# rf_model = RandomForestClassifier(n_estimators=1000, max_depth = 10, criterion = 'entropy', random_state = 42)
# rf_model.fit(X_train, y_train)
# rf_hat = rf_model.predict(X_test)
# lgb_model = lgb.LGBMClassifier(n_estimators=500, max_depth = 6, learning_rate = 0.01,random_state = 42)
# lgb_model.fit(X_train, y_train)
# lgb_hat = lgb_model.predict(X_test)
# xgb_model = xgb.XGBClassifier(n_estimators=250, max_depth = 2, learning_rate = 0.1, random_state = 42)
# xgb_model.fit(X_train, y_train)
# xgb_hat = xgb_model.predict(X_test)
# svm_model = svm.SVC(C=1, kernel = "rbf", degree = 3, random_state = 42)
# svm_model.fit(X_train, y_train)
# svm_hat = svm_model.predict(X_test)
# ada_model = AdaBoostClassifier(n_estimators=100, algorithm = "SAMME.R", learning_rate = 0.01, random_state = 42)
# ada_model.fit(X_train, y_train)
# ada_hat = ada_model.predict(X_test)
# # eclf1 = VotingClassifier(estimators=[('rf', rf_model), ('lgb', lgb_model), ('xgb', xgb_model), ('ada', ada_model),('svm', svm_model)], voting='soft')
# # eclf1 = eclf1.fit(X_train, y_train)
# # eclf1_hat = eclf1.predict(X_test)

# rf_decision = rf_model.predict_proba(X_test)
# lgb_decision = lgb_model.predict_proba(X_test)
# xgb_decision = xgb_model.predict_proba(X_test)
# svm_decision = svm_model.decision_function(X_test)
# ada_decision = ada_model.predict_proba(X_test)

# eclf2 = VotingClassifier(estimators=[('rf', rf_model), ('lgb', lgb_model), ('xgb', xgb_model)], voting='soft')
# eclf2 = eclf2.fit(X_train, y_train)
# eclf2_hat = eclf2.predict(X_test)

# majority_vote_decision = eclf2.predict_proba(X_test)

# Y_test = label_binarize(y_test, classes=xgb_model.classes_)

# n_classes = Y_test.shape[1]

# decisions = [rf_decision,lgb_decision,xgb_decision,svm_decision, ada_decision, majority_vote_decision]
# models = ['Random Forest','LightGBM','XGBoost','SVM', 'Ada', 'Majority vote']
# def precision_recall(decision,model):
#   precision = dict()
#   recall = dict()
#   average_precision = dict()
#   for i in range(n_classes):
#       precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i], decision[:, i])
#       average_precision[i] = average_precision_score(Y_test[:, i], decision[:, i])

#   # A "micro-average": quantifying score on all classes jointly
#   precision["micro"], recall["micro"], _ = precision_recall_curve(
#       Y_test.ravel(), decision.ravel()
#   )
#   average_precision["micro"] = average_precision_score(Y_test, decision, average="micro")
#   plt.plot(recall["micro"], precision["micro"], label = model)

# plt.figure(dpi=200)
# for decision, model in zip(decisions, models):
#   precision_recall(decision,model)
#   plt.legend()
# plt.savefig('/content/gdrive/MyDrive/Colab Notebooks/summer/realtime/precision_recall_curve.jpg',bbox_inches="tight")

