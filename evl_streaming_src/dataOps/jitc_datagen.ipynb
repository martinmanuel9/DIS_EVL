{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/docker/users/martinmlopez/DIS_EVL/data/testJITC\n",
      "/srv/docker/users/martinmlopez/DIS_EVL/data/JITC_Data/\n",
      "(4000, 241960, 1) (1000, 241960, 1) (4000,) (1000,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">241960</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m241960\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m10,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,451</span> (40.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,451\u001b[0m (40.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,451</span> (40.82 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,451\u001b[0m (40.82 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m 14/100\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:10:53\u001b[0m 175s/step - accuracy: 0.4933 - loss: 0.6933"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Application:        JITC processing\n",
    "File name:          jitc_datagen.py\n",
    "Author:             Martin Manuel Lopez\n",
    "Creation:           05/06/2024\n",
    "\n",
    "The University of Arizona\n",
    "Department of Electrical and Computer Engineering\n",
    "College of Engineering\n",
    "\"\"\"\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class JITC_DATAOPS:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.data = {}  # dict to store processed data\n",
    "        self.dataframe = pd.DataFrame()\n",
    "        self.import_data()\n",
    "\n",
    "    ## Step 1: Get all json files from onedrive and place them in a single directory\n",
    "    ## Step 2: Run this once once you download all files and put them in a single directory\n",
    "    def update_jsons(self, directory):\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.json'):\n",
    "                #prepend and append {} of each file\n",
    "                with open(directory + '/' + filename, 'r') as f:\n",
    "                    data = f.read()\n",
    "                    data = '{\\\"binary\\\":' + data + '}'\n",
    "                with open(directory + '/' + filename, 'w') as f:\n",
    "                    f.write(data)\n",
    "\n",
    "    ## --------------------------------------------------------------\n",
    "\n",
    "    def change_directory(self):\n",
    "        path = os.getcwd()\n",
    "        ###  debug mode ---------------------------\n",
    "        # testPath = str(path) + '/data/JITC_Data/'\n",
    "        # os.chdir(testPath)\n",
    "        #------------------------------------------\n",
    "        ### run mode: change path to data directory\n",
    "        path = Path(path)\n",
    "        print(path)\n",
    "        path = path.parents[1]\n",
    "        changed_path = str(path) + '/data/JITC_Data/'\n",
    "        print(changed_path)\n",
    "        os.chdir(changed_path)\n",
    "\n",
    "    # add binaries into a list\n",
    "    def process_directory(self, directory):\n",
    "        # print(\"Processing directory:\", directory)\n",
    "        # need to prepend each file with {} to make json file correct\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith('.json'):\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                self.process_json_file(filepath)\n",
    "\n",
    "    def process_json_file(self, json_file):\n",
    "        with open(json_file, 'r') as f:\n",
    "            json_data = json.load(f)  # Load JSON data\n",
    "            # print(json_data['binary'])\n",
    "            self.data[os.path.basename(json_file)] = json_data['binary']\n",
    "\n",
    "    def import_data(self):\n",
    "        self.change_directory()\n",
    "        self.process_directory(os.getcwd())\n",
    "        self.dataframe = pd.DataFrame.from_dict(self.data, orient='index', columns=['binary'])\n",
    "        self.dataframe.index.name = 'filename'\n",
    "\n",
    "    def develop_dataset(self):\n",
    "        # Convert binary strings to lists of integers\n",
    "        self.dataframe['binary'] = self.dataframe['binary'].apply(lambda x: [int(b) for b in x])\n",
    "\n",
    "        # Pad sequences to the same length\n",
    "        sequences = pad_sequences(self.dataframe['binary'].tolist(), padding='post')\n",
    "\n",
    "        # Assuming binary labels for the LSTM model (sum of sequence mod 2 as an example)\n",
    "        labels = np.array([sum(seq) % 2 for seq in sequences])\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Reshape for LSTM [samples, time steps, features]\n",
    "        X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "        X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def build_lstm_model(self, input_shape):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = LSTM(50, recurrent_activation='sigmoid', recurrent_dropout=0)(inputs)\n",
    "        outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "        model = Model(inputs, outputs)\n",
    "        model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        feature_extractor = Model(inputs, x)  # Model to extract features from the LSTM layer\n",
    "        return model, feature_extractor\n",
    "\n",
    "    def train_and_evaluate_model(self, model, X_train, X_test, y_train, y_test):\n",
    "        history = model.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.2) # used to be 20 epochs\n",
    "\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        print(f'Test Accuracy: {accuracy:.2f}')\n",
    "        return history\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataOps = JITC_DATAOPS(dataset='JITC')\n",
    "    #### run the following only once to update json files\n",
    "    # dataOps.update_jsons(os.getcwd() + '/data/JITC_Data')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = dataOps.develop_dataset()\n",
    "    \n",
    "    \n",
    "    # make new directory called artifacts and change to that directory\n",
    "    os.mkdir('artifacts')\n",
    "    os.chdir('artifacts')\n",
    "    \n",
    "    # save X_train, X_test, y_train, y_test in pickle and h5 format\n",
    "    X_train = pd.DataFrame(X_train.reshape(X_train.shape[0], -1))\n",
    "    X_test = pd.DataFrame(X_test.reshape(X_test.shape[0], -1))\n",
    "    \n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "    \n",
    "    \n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    y_train = pd.DataFrame(y_train)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    X_train.to_pickle('X_train.pkl')\n",
    "    X_test.to_pickle('X_test.pkl')\n",
    "    y_train.to_pickle('y_train.pkl')\n",
    "    y_test.to_pickle('y_test.pkl')\n",
    "    \n",
    "\n",
    "    lstm_model, feature_extractor = dataOps.build_lstm_model((X_train.shape[1], 1))\n",
    "    history = dataOps.train_and_evaluate_model(lstm_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Extract features from the LSTM layer\n",
    "    X_train_features = feature_extractor.predict(X_train)\n",
    "    X_test_features = feature_extractor.predict(X_test)\n",
    "\n",
    "    # os.chdir('/..')\n",
    "    print(os.getcwd())\n",
    "    # save features in pickle and h5 format\n",
    "    X_train_features = pd.DataFrame(X_train_features)\n",
    "    X_test_features = pd.DataFrame(X_test_features)\n",
    "    X_train_features.to_pickle('X_train_features.pkl')\n",
    "    X_test_features.to_pickle('X_test_features.pkl')\n",
    "    # X_train_features.to_hdf('X_train_features.h5', key='X_train_features')\n",
    "    # X_test_features.to_hdf('X_test_features.h5', key='X_test_features')\n",
    "\n",
    "\n",
    "    # Train a different model (e.g., RandomForest) using the extracted features\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train_features, y_train)\n",
    "    y_pred = rf_model.predict(X_test_features)\n",
    "    rf_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Random Forest Test Accuracy: {rf_accuracy:.2f}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
